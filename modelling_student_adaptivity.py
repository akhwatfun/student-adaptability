# -*- coding: utf-8 -*-
"""Modelling Student Adaptivity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ljzAhxaVuqzF9QhmQoTyYGskoAQiqZAt
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import math
import random
import seaborn as sns
from scipy.stats import pearsonr, jarque_bera
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report, confusion_matrix, f1_score
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings("ignore")

# %matplotlib inline

# Memuat dataset yang sudah rapi ditahapan sebelumnya
data = "data.csv"
df = pd.read_csv(data, index_col=0)

df['Gender'] = df['Gender'].replace({'Boy': 1,
                                   'Girl': 0})
df['Age'] = df['Age'].replace({'1-5': 0,
                                '6-10': 1,
                                '11-15': 2,
                                '16-20': 3,
                                '21-25': 4,
                                '26-30': 5})
df['Education Level'] = df['Education Level'].replace({'School': 0,
                                       'College': 1,
                                       'University': 2})
df['Institution Type'] = df['Institution Type'].replace({'Non Government': 0,
                                       'Government': 1})
df['IT Student'] = df['IT Student'].replace({'No': 0,
                                       'Yes': 1})
df['Location'] = df['Location'].replace({'Yes': 1,
                                   'No': 0})
df['Load-shedding'] = df['Load-shedding'].replace({'Low': 0,
                                   'High': 1})
df['Financial Condition'] = df['Financial Condition'].replace({'Poor': 1,
                                     'Mid': 2,
                                     'Rich': 3})
df['Internet Type'] = df['Internet Type'].replace({'Mobile Data': 1,
                                   'Wifi': 2})
df['Network Type'] = df['Network Type'].replace({'2G': 1,
                                     '3G': 2,
                                     '4G': 3})
df['Class Duration'] = df['Class Duration'].replace({'0': 0,
                                  '1-3': 1,
                                  '3-6': 2})
df['Self LMS'] = df['Self LMS'].replace({'No': 0,
                                       'Yes': 1})
df['Device'] = df['Device'].replace({'Tab': 1,
                                     'Mobile': 2,
                                     'Computer': 3})
df['Adaptivity Level'] = df['Adaptivity Level'].replace({'Low': 0,
                                         'Moderate': 1,
                                         'High': 2})

# Memasukkan 7 variabel terpilih
variabel = ["Institution Type",  "Location",
                "Load-shedding", "Financial Condition", "Internet Type", "Network Type", "Class Duration"
                ]
analyze = df[variabel]

# Liat informasi dari dataset yang kita miliki
print(df.head())
print(df.columns)

"""**Memisahkan variabel dependen dan independen**

Variabel dependen yang digunakan adalah data pada kolom 'target', kolom feature merupakan variabel independen.
"""

# Define X and y
X = df.loc[:, ~df.columns.isin(["Adaptivity Level"])]
y = df["Adaptivity Level"]

# Melihat kolom yang termasuk variable indepnden
X.head()

# Melihat variabel dependen
y.head()

# Mendefinisikan standard scaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled

"""**Spliting data**

Pisahkan dataset menjadi training dan testing dataset dengan perbandingan 80:20 dan random state 42!
"""

# Mendefinisikan pemisahan data
X_train, X_test, y_train, y_test = train_test_split(X_scaled , y, test_size=0.2, random_state=100)

# Menampilkan hasil pemisahan berdasarkan kolom dan baris
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""**Pemodelan dengan Student Adaptivity Dataset**

Kita akan melakukan pemodelan untuk 4 model berbeda diantaranya logistic regression, decision tree, random forest dan neural networks.
"""

# Menyiapkan libraries
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# pengalian informasi Logistik Regresi dan hitung akurasinya
clf = LogisticRegression()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of Logistric Regression Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi Decision Tree dan hitung akurasinya
clf = DecisionTreeClassifier()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of Decision Tree Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi Random Forest dan hitung akurasinya
clf = RandomForestClassifier()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of Random Forest Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi MLP dan hitung akurasinya
clf = MLPClassifier()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of MLP Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi XGBClassifier dan hitung akurasinya
clf = XGBClassifier()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of XGB Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi KNeighborsClassifier dan hitung akurasinya
clf = KNeighborsClassifier(n_neighbors=5)
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of KNeighbors Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi QuadraticDiscriminantAnalysis dan hitung akurasinya
clf = QuadraticDiscriminantAnalysis()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of QuadraticDiscriminantAnalysis Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi GaussianNB dan hitung akurasinya
clf = GaussianNB()
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of GaussianNB Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

# pengalian informasi SVC dan hitung akurasinya
clf = SVC(kernel='linear')
# train the classifier
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# printing the test accuracy
print("The test accuracy score of SVC Classifier is ", accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

"""**Membandingkan hasil diatas**

Logistric Regression Classifier akurasi 0.60

Decision Tree Classifier akurasi 0.52

Random Forest Classifier akurasi 0.54

MLP Classifier akurasi 0.54

XGB Classifier akurasi 0.58

KNeighbors Classifier akurasi 0.54

QuadraticDiscriminantAnalysis Classifier akurasi 0.58

GaussianNB Classifier akurasi 0.54

SVC Classifier akurasi 0.62

Dari data di atas, kita akan analisa lebih lanjut Logistric Regression Classifier, XGB Classifier, QuadraticDiscriminantAnalysis Classifier, SVC Classifier

**Proses Hyperparameter Tuning**
"""

# Logistic regression classifier with hyperparameter
clf = LogisticRegression()

param_grid1 = {
    'max_iter': [30, 50, 100, 150],
    'multi_class': ['auto'],
    'solver' : ['lbfgs', 'newton-cholesky']
}

gs1 = GridSearchCV(
        estimator= clf,
        param_grid = param_grid1,
        cv=5,
        n_jobs=-1,
        scoring='accuracy'
    )

fit_clf_lg = gs1.fit(X_train, y_train)

print(fit_clf_lg.best_score_)
print(fit_clf_lg.best_params_)

y_pred = fit_clf_lg.predict(X_test)
print(classification_report(y_test, y_pred))

# XGBClassifier with hyperparameter
clf = XGBClassifier(random_state=42)

param_grid1 = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

gs1 = GridSearchCV(
        estimator= clf,
        param_grid = param_grid1,
        cv=5,
        n_jobs=-1,
        scoring='accuracy'
    )

fit_clf_xgb = gs1.fit(X_train, y_train)

print(fit_clf_xgb.best_score_)
print(fit_clf_xgb.best_params_)

y_pred = fit_clf_xgb.predict(X_test)
print(classification_report(y_test, y_pred))

# QuadraticDiscriminantAnalysis with hyperparameter
clf = QuadraticDiscriminantAnalysis()

param_grid1 = {}

gs1 = GridSearchCV(
        estimator= clf,
        param_grid = param_grid1,
        cv=5,
        n_jobs=-1,
        scoring='accuracy'
    )

fit_clf_qda = gs1.fit(X_train, y_train)

print(fit_clf_qda.best_score_)
print(fit_clf_qda.best_params_)

y_pred = fit_clf_qda.predict(X_test)
print(classification_report(y_test, y_pred))

# SVC classifier with hyperparameter
clf = SVC(probability=True)

param_grid1 = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto'],
    'degree': [2, 3, 4]
}

gs1 = GridSearchCV(
        estimator= clf,
        param_grid = param_grid1,
        cv=5,
        n_jobs=-1,
        scoring='accuracy'
    )

fit_clf_svc = gs1.fit(X_train, y_train)

print(fit_clf_svc.best_score_)
print(fit_clf_svc.best_params_)

y_pred = fit_clf_svc.predict(X_test)
print(classification_report(y_test, y_pred))

"""**ROC Analysis**

Kurva yang menyajikan ilustrasi performansi dari binary classifier system dalam menghasilkan sebuah prediksi
"""

# Predict the probabilities for the positive class
y_pred_logreg = fit_clf_lg.predict_proba(X_test)[:, 1]
y_pred_xgb = fit_clf_xgb.predict_proba(X_test)[:, 1]
y_pred_qda= fit_clf_qda.predict_proba(X_test)[:, 1]
y_pred_svc= fit_clf_svc.predict_proba(X_test)[:, 1]

# Calculate the AUC-ROC scores
auc_logreg = roc_auc_score(y_test, y_pred_logreg)
auc_xgb = roc_auc_score(y_test, y_pred_xgb)
auc_qda = roc_auc_score(y_test, y_pred_qda)
auc_svc = roc_auc_score(y_test, y_pred_svc)

print(f"AUC-ROC for Logistic Regression: {auc_logreg}")
print(f"AUC-ROC for XGBoost: {auc_xgb}")
print(f"AUC-ROC for QuadraticDiscriminantAnalysis: {auc_qda}")
print(f"AUC-ROC for SVC: {auc_svc}")

#Membuat ROC Analisis
def plot_roc_curves(y_test, y_pred_logreg, y_pred_xgb, y_pred_qda, y_pred_svc):
    plt.figure(figsize=(8, 6))

    # Calculate ROC curves for each model
    fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_pred_logreg)
    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_xgb)
    fpr_qda, tpr_qda, _ = roc_curve(y_test, y_pred_qda)
    fpr_svc, tpr_svc, _ = roc_curve(y_test, y_pred_svc)

    # Plot ROC curves
    plt.plot(fpr_logreg, tpr_logreg, label=f'Logistic Regression (AUC = {auc_logreg:.2f})')
    plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.2f})')
    plt.plot(fpr_qda, tpr_qda, label=f'QuadraticDiscriminantAnalysis (AUC = {auc_qda:.2f})')
    plt.plot(fpr_svc, tpr_svc, label=f'SVC (AUC = {auc_svc:.2f})')

    # Plot random classifier
    plt.plot([0,1], [0,1], linestyle="--", color="gray")

    # Format the plot
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curves for Student Adaptivity Level')
    plt.legend()
    plt.show()

plot_roc_curves(y_test, y_pred_logreg, y_pred_xgb, y_pred_qda, y_pred_svc)

#ROC Analysis Graph untuk menentukan treshold'''
def find_rates_for_thresholds(y_test, y_pred, threshold):
    fpr_list = []
    tpr_list = []
    for threshold in thresholds:
        y_pred_binary = (y_pred > threshold).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()
        fpr = fp / (fp + tn)
        tpr = tp / (tp + fn)
        fpr_list.append(fpr)
        tpr_list.append(tpr)
    return fpr_list, tpr_list

thresholds = np.arange(0, 1.1, 0.1)

fpr_logreg, tpr_logreg = find_rates_for_thresholds(y_test, y_pred_logreg, thresholds)
fpr_xgb, tpr_xgb = find_rates_for_thresholds(y_test, y_pred_xgb, thresholds)
fpr_qda, tpr_qda = find_rates_for_thresholds(y_test, y_pred_qda, thresholds)
fpr_svc, tpr_svc = find_rates_for_thresholds(y_test, y_pred_svc, thresholds)

'''TODO: Silahkan buat Summary DataFrame'''
summary_df = pd.DataFrame({
    'Threshold' : thresholds,
    'FPR_logreg' : fpr_logreg,
    'FPR_XGB' : fpr_xgb,
    'FPR_QDA' : fpr_qda,
    'FPR_SVC' : fpr_svc,
    'TPR_logreg' : tpr_logreg,
    'TPR_XGB' : tpr_xgb,
    'TPR_QDA' : tpr_qda,
    'TPR_SVC' : tpr_svc,
})

print(summary_df)

#ROC Analysis Graph untuk menentukan best treshold'''
def find_best_threshold(y_test, y_pred):
    # based on Youden's Index
    fpr, tpr, thresholds = roc_curve(y_test, y_pred)
    optimal_idx = np.argmax(tpr - fpr)
    return thresholds[optimal_idx]

best_threshold_logreg = find_best_threshold(y_test, y_pred_logreg)
best_threshold_xgb = find_best_threshold(y_test, y_pred_xgb)
best_threshold_qda = find_best_threshold(y_test, y_pred_qda)
best_threshold_svc = find_best_threshold(y_test, y_pred_svc)

print(f"Best threshold for Logistic Regression: {best_threshold_logreg}")
print(f"Best threshold for XGBoost: {best_threshold_xgb}")
print(f"Best threshold for QuadraticDiscriminantAnalysis: {best_threshold_qda}")
print(f"Best threshold for SVC: {best_threshold_svc}")

import pickle
# Menyimpan model terbaik dengan pickle
pklname = "best_model_logreg.pkl"

with open(pklname, 'wb') as file:
    pickle.dump(fit_clf_lg, file)